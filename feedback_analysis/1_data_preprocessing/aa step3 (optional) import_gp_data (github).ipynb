{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "zakładka: z/f?\n",
      "strona numer: 1\n",
      "strona numer: 2\n",
      "strona numer: 3\n",
      "strona numer: 4\n",
      "strona numer: 5\n",
      "strona numer: 6\n",
      "strona numer: 7\n",
      "strona numer: 8\n",
      "strona numer: 9\n",
      "zakładka: z/g?\n",
      "strona numer: 1\n",
      "strona numer: 2\n",
      "strona numer: 3\n",
      "strona numer: 4\n",
      "strona numer: 5\n",
      "strona numer: 6\n",
      "strona numer: 7\n",
      "strona numer: 8\n",
      "strona numer: 9\n",
      "strona numer: 10\n",
      "strona numer: 11\n",
      "strona numer: 12\n",
      "strona numer: 13\n",
      "strona numer: 14\n",
      "strona numer: 15\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\n",
      "\n",
      "koniec\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "page = requests.get('http://v1.syndication.nhschoices.nhs.uk/organisations/gppractices/atoz?apikey=JROLOVFO')\n",
    "tree = html.fromstring(page.content)\n",
    "links = tree.xpath('//li/a/@href')\n",
    "\n",
    "if os.path.isfile(\"gp_details.csv\") == True:\n",
    "    os.remove(\"gp_details.csv\")\n",
    "\n",
    "\n",
    "#     PODSTAWOWA FUNKCJA POZWALAJĄCA NA ŚCIĄGNIĘCIE WŁAŚCIWYCH INFORMACJI Z PODSTRON O GP PRACTICES\n",
    "def kielbaska (one_gp_link):\n",
    "    pytania = ['telephone?', ' want one?', 'd respect?', 'treatment?', 'ning hours']\n",
    "    kielbasa = []\n",
    "    gp_page = requests.get(one_gp_link)\n",
    "    tree4 = html.fromstring(gp_page.content)\n",
    "    odpowiedzi = tree4.xpath('//span[@class=\"value\" or @class=\"minValue\" or @class=\"maxValue\" or @class=\"numberOfRatings\"]/text()')\n",
    "    odpowiedzi = [re.sub(' +','', x.replace('\\r\\n', '')) for x in odpowiedzi]\n",
    "    if len(odpowiedzi) == 20:\n",
    "            kielbasa.extend(odpowiedzi)\n",
    "    elif len(odpowiedzi) > 0:\n",
    "#         print 'UWAGA: niestandardowa liczba odpowiedzi na pytania ankietowe: ' + str(len(odpowiedzi))\n",
    "        zapytania = tree4.xpath('//div[@class=\"questionText\"]/text()')\n",
    "        podane_odpowiedzi = []\n",
    "        for x in zapytania:\n",
    "            x = re.sub(' +',' ', x.replace('\\r\\n', ''))\n",
    "            if x[-10:] in pytania:\n",
    "                starter = pytania.index(x[-10:])*4\n",
    "                koniec = starter + 4\n",
    "                podane_odpowiedzi.extend(range(starter,koniec))\n",
    "            else:\n",
    "                print False\n",
    "                print 'nieznane pytanie: ' + str(x)\n",
    "        pakunek = []\n",
    "        licznik = 0\n",
    "        for x in range(20):\n",
    "            if x in podane_odpowiedzi:\n",
    "                pakunek.append(odpowiedzi[licznik])\n",
    "                licznik += 1\n",
    "            else:\n",
    "                pakunek.append('')\n",
    "        kielbasa.extend(pakunek)\n",
    "    else:\n",
    "        kielbasa.extend(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''])\n",
    "\n",
    "    tempik = gp_link.find(\"?apikey\")\n",
    "#   scraping z podstrony \"overview\"\n",
    "    overview_link = gp_link[:tempik] + '/overview' + gp_link[tempik:]\n",
    "    overview_page = requests.get(overview_link)\n",
    "    tree_overview = html.fromstring(overview_page.content)\n",
    "\n",
    "    overv1 = tree_overview.xpath('//div[@id=\"MainContent\"]//dd[1]/text()')\n",
    "    overv2 = tree_overview.xpath('//div[@id=\"MainContent\"]//dd[2]/text()')\n",
    "    kielbasa.append(overv1[0])\n",
    "    kielbasa.append(overv2[0])\n",
    "    kielbasa.append(overv1[1])\n",
    "    kielbasa.append(overv2[1])\n",
    "    \n",
    "    overv2_2 = tree_overview.xpath('//div[@id=\"MainContent\"]//*[text()=\"GPs at this practice:\"]/following-sibling::dd[1]/text()')\n",
    "    if len(overv2_2) != 0:\n",
    "        lekarze = re.sub(' +',' ', overv2_2[0].replace('\\r\\n', ''))\n",
    "        kielbasa.append(lekarze[1:-1])\n",
    "    else: \n",
    "        kielbasa.append('')\n",
    "    overv3 = tree_overview.xpath('//ul[@class=\"address\"]//li/text()')\n",
    "    kielbasa.extend(overv3[-3:])\n",
    "    overv4 = tree_overview.xpath('//div[@id=\"MainContent\"]//h2/text()')\n",
    "    nazwa = overv4[0]\n",
    "    try:\n",
    "        nazwa.decode('ascii')\n",
    "    except UnicodeEncodeError:\n",
    "        nazwa = re.sub(u\"\\u2013\", \"-\", overv4[0])\n",
    "        nazwa2 = re.sub(u\"\\u2019\", \"'\", nazwa)\n",
    "        kielbasa.insert(0,nazwa2[:-11])\n",
    "    else:\n",
    "        kielbasa.insert(0,nazwa[:-11])\n",
    "    info_overv = tree_overview.xpath('//div[@id=\"MainContent\"]/descendant::text()')\n",
    "    overv5 = tree_overview.xpath('//div[@id=\"MainContent\"]//p/text()')\n",
    "    obiekt = []\n",
    "    for x in overv5:\n",
    "        if x[:6] == 'Last v':\n",
    "            obiekt.append(x[17:])\n",
    "    if len(obiekt) == 2:\n",
    "        kielbasa.extend(obiekt)\n",
    "    elif len(obiekt) < 2:\n",
    "        if len(obiekt) == 1:\n",
    "            kielbasa.extend(obiekt)\n",
    "            kielbasa.append('')\n",
    "        else:\n",
    "            kielbasa.append('')\n",
    "            kielbasa.append('')\n",
    "    else:\n",
    "        print \"UWAGA. Liczba wpisów 'last verified' jest większa niż 2\"\n",
    "#     TUTAJ TRZEBA COŚ PRZEROBIĆ ŻEBY PRAWIDŁOWO IMIONA I NAZWISKA LEKARZY SIĘ ZAPISYWAŁY\n",
    "\n",
    "#     overv6 = tree_overview.xpath('//div[@id=\"MainContent\"]//dd[1]//ul//li/text()')\n",
    "    overv6 = tree_overview.xpath('//div[@id=\"MainContent\"]//*[text()=\"Doctors:\"]/following-sibling::dd[1]//ul//li/text()')\n",
    "    overv6_mod = []\n",
    "    for item in overv6:\n",
    "        if item != 'Closed':\n",
    "            overv6_mod.append(item)\n",
    "    kielbasa.append(overv6_mod)\n",
    "    overv7 = tree_overview.xpath('//div[@id=\"MainContent\"]//*[text()=\"Is Electronic Prescription Service (EPS) available:\"]/following-sibling::dd[1]/text()')\n",
    "    kielbasa.extend(overv7)\n",
    "    overv8 = tree_overview.xpath('//div[@id=\"MainContent\"]//h2[1]/following-sibling::dl[1]//*/text()')\n",
    "    overv8 = [re.sub(' +','', x.replace('\\r\\n', '')) for x in overv8]\n",
    "#     print\n",
    "#     print 'kiełbasa: ' + kielbasa[0]\n",
    "\n",
    "    r_times = 0\n",
    "    s_times = 0\n",
    "#     out_hours = 0\n",
    "    for aa, xx in enumerate(overv8):\n",
    "        if len(xx) > 0:\n",
    "            if xx[:10] == \"Receptiont\":\n",
    "                r_times = aa\n",
    "            elif xx[:10] == \"Surgerytim\":\n",
    "                s_times = aa\n",
    "#             elif xx[:10] == \"Outofhours\":\n",
    "#                 out_hours = aa\n",
    "#     print\n",
    "#     print 'overv8'\n",
    "#     print overv8\n",
    "#     print 's_times'\n",
    "#     print s_times\n",
    "#     print 'r_times'\n",
    "#     print r_times\n",
    "    recep_list = []\n",
    "    surg_list = []\n",
    "    for aa, xx in enumerate(overv8):\n",
    "        if aa > r_times and aa < s_times:\n",
    "            if len(xx) > 0:\n",
    "                recep_list.append(xx)\n",
    "        elif aa > s_times:\n",
    "            if len(xx) > 0:\n",
    "                surg_list.append(xx)        \n",
    "#     for aa, xx in enumerate(overv8):\n",
    "#         if out_hours != 0:\n",
    "#             if aa > r_times and aa < s_times:\n",
    "#                 if len(xx) > 0:\n",
    "#                     recep_list.append(xx)\n",
    "#             elif aa > s_times and aa < out_hours:\n",
    "#                 if len(xx) > 0:\n",
    "#                     surg_list.append(xx)\n",
    "#         else:\n",
    "#             if aa > r_times and aa < s_times:\n",
    "#                 if len(xx) > 0:\n",
    "#                     recep_list.append(xx)\n",
    "#             elif aa > s_times:\n",
    "#                 if len(xx) > 0:\n",
    "#                     surg_list.append(xx)\n",
    "#     print 'recep_list'\n",
    "#     print recep_list\n",
    "#     print 'surg_list'\n",
    "#     print surg_list\n",
    "    if r_times != 0:\n",
    "        recep_times = []\n",
    "        week_recep = []\n",
    "        sun = range(recep_list.index(\"Sunday\")+1,recep_list.index(\"Monday\"))\n",
    "        mon = range(recep_list.index(\"Monday\")+1,recep_list.index(\"Tuesday\"))\n",
    "        tue = range(recep_list.index(\"Tuesday\")+1,recep_list.index(\"Wednesday\"))\n",
    "        wed = range(recep_list.index(\"Wednesday\")+1,recep_list.index(\"Thursday\"))\n",
    "        thu = range(recep_list.index(\"Thursday\")+1,recep_list.index(\"Friday\"))\n",
    "        fri = range(recep_list.index(\"Friday\")+1,recep_list.index(\"Saturday\"))\n",
    "        sat = range(recep_list.index(\"Saturday\")+1,len(recep_list))\n",
    "        week_recep.append(sun)\n",
    "        week_recep.append(mon)\n",
    "        week_recep.append(tue)\n",
    "        week_recep.append(wed)\n",
    "        week_recep.append(thu)\n",
    "        week_recep.append(fri)\n",
    "        week_recep.append(sat)\n",
    "        for x in week_recep:\n",
    "            temp = []\n",
    "            for item in x:\n",
    "                temp.append(recep_list[item])\n",
    "            recep_times.append(temp)\n",
    "        kielbasa.extend(recep_times)\n",
    "    else:\n",
    "        kielbasa.extend(['','','','','','',''])\n",
    "    if s_times != 0:\n",
    "        surg_list2 = []\n",
    "        maksimo = len(surg_list)\n",
    "        for aa, xx in enumerate(surg_list):\n",
    "            if aa <= surg_list.index('Saturday'):\n",
    "                surg_list2.append(xx)\n",
    "            else:\n",
    "                if xx[0] in ['0','1','2','C'] and aa <= maksimo:\n",
    "                    surg_list2.append(xx)\n",
    "                else:\n",
    "                    maksimo = aa\n",
    "        week_surg = []\n",
    "        surg_times = []\n",
    "        sun_s = range(surg_list2.index(\"Sunday\")+1,surg_list2.index(\"Monday\"))\n",
    "        mon_s = range(surg_list2.index(\"Monday\")+1,surg_list2.index(\"Tuesday\"))\n",
    "        tue_s = range(surg_list2.index(\"Tuesday\")+1,surg_list2.index(\"Wednesday\"))\n",
    "        wed_s = range(surg_list2.index(\"Wednesday\")+1,surg_list2.index(\"Thursday\"))\n",
    "        thu_s = range(surg_list2.index(\"Thursday\")+1,surg_list2.index(\"Friday\"))\n",
    "        fri_s = range(surg_list2.index(\"Friday\")+1,surg_list2.index(\"Saturday\"))\n",
    "        sat_s = range(surg_list2.index(\"Saturday\")+1,len(surg_list2))\n",
    "        week_surg.append(sun_s)\n",
    "        week_surg.append(mon_s)\n",
    "        week_surg.append(tue_s)\n",
    "        week_surg.append(wed_s)\n",
    "        week_surg.append(thu_s)\n",
    "        week_surg.append(fri_s)\n",
    "        week_surg.append(sat_s)\n",
    "        for x in week_surg:\n",
    "            temp = []\n",
    "            for item in x:\n",
    "                temp.append(surg_list2[item])\n",
    "            surg_times.append(temp)\n",
    "        kielbasa.extend(surg_times)\n",
    "    else:\n",
    "        kielbasa.extend(['','','','','','',''])\n",
    "    overv9 = tree_overview.xpath('//div[@id=\"MainContent\"]//dl//*[text()=\"Parent organisation:\"]/following-sibling::dd[1]//a/text()')\n",
    "    kielbasa.extend(overv9)\n",
    "#   scraping z podstrony \"Services\"\n",
    "    services_link = gp_link[:tempik] + '/services' + gp_link[tempik:]\n",
    "    services_page = requests.get(services_link)\n",
    "    tree_services = html.fromstring(services_page.content)\n",
    "    serv1 = tree_services.xpath('''//div[@id=\"MainContent\"]//ul[@class=\"services\"]//li//dd//a/text()''')\n",
    "    kielbasa.append(serv1)\n",
    "#   scraping z podstrony \"Facilities\"\n",
    "    facilities_link = gp_link[:tempik] + '/facilities' + gp_link[tempik:]\n",
    "    facilities_page = requests.get(facilities_link)\n",
    "    tree_facilities = html.fromstring(facilities_page.content)\n",
    "    facil1 = tree_facilities.xpath('//div[@id=\"MainContent\"]//dl//dd//ul//li//dl//dd//ul//li/text()')\n",
    "    facils = []\n",
    "    for x in facil1:\n",
    "        if x[-1] == 's':\n",
    "            facils.append(x[:-6])\n",
    "    kielbasa.append(facils)\n",
    "#   scraping z podstrony \"treatment indicators\"\n",
    "    treatments_link = gp_link[:tempik] + '/indicators/treatment' + gp_link[tempik:]\n",
    "    treatments_page = requests.get(treatments_link)\n",
    "    tree_treatments = html.fromstring(treatments_page.content)\n",
    "    treat1 = tree_treatments.xpath('//div[@id=\"MainContent\"]//ul[@id=\"links\"]//li//a/text()')\n",
    "    treat1 = [re.sub(' +',' ', x.replace('\\r\\n', '')) for x in treat1]\n",
    "    l1 = [a.start() for a in re.finditer(' - ', treat1[1])]\n",
    "    l2 = [a.start() for a in re.finditer(' - ', treat1[2])]\n",
    "    l3 = [a.start() for a in re.finditer(' - ', treat1[4])]\n",
    "    cut_off = 0\n",
    "    for x in range(10):\n",
    "        try:\n",
    "            if l1[x] == l2[x] and l1[x] == l3[x]:\n",
    "                cut_off = l1[x] + 3\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "    treatments = []\n",
    "    for row in treat1:\n",
    "        treatments.append(row[cut_off:])\n",
    "    kielbasa.append(treatments)\n",
    "    return kielbasa\n",
    "# #                     scraping z podstrony \"staff\"\n",
    "#                     staff_link = gp_link[:tempik] + '/staff' + gp_link[tempik:]\n",
    "#                     staff_page = requests.get(staff_link)\n",
    "#                     tree_staff = html.fromstring(staff_page.content)\n",
    "#                     print staff_page.content\n",
    "# #                 OGÓLNE INFORMACJE O PRACOWNIKACH\n",
    "#                     staff1 = tree_staff.xpath('//div[@id=\"MainContent\"]//ul//li/text()')\n",
    "\n",
    "# #                 KATEGORIE PRACOWNIKÓW\n",
    "#                     staff2 = tree_staff.xpath('//div[@id=\"MainContent\"]//dl[2]//dt/text()')\n",
    "    \n",
    "#                     staff3 = tree_staff.xpath('//div[@id=\"MainContent\"]//dl[2]//dd//ul//li/text()')\n",
    "#                     staff_list = []\n",
    "#                     for cat in staff2: \n",
    "                \n",
    "# #                     scraping z podstrony \"core indicators\"\n",
    "#                     core_link = gp_link[:tempik] + '/indicators/core' + gp_link[tempik:]\n",
    "#                     core_page = requests.get(core_link)\n",
    "#                     tree_core = html.fromstring(core_page.content)\n",
    "# #                     core1 = tree_core.xpath('//div[@id=\"MainContent\"]//ul//li//dl//*[text()=\"Value:\"]/following::*[1]/text()')\n",
    "#                     core1 = tree_core.xpath('//div[@id=\"MainContent\"]//ul//li/node()')\n",
    "\n",
    "#                     print kielbasa\n",
    "\n",
    "\n",
    "\n",
    "with open('gp_details.csv', 'ab') as dok:\n",
    "    writer = csv.writer(dok, delimiter=',')\n",
    "    row0 = ['gp_name', 'telph_access_avg', 'telph_access_min', 'telph_access_max', \n",
    "            'telph_access_count', 'preferred_apptmnt_time_avg', 'preferred_apptmnt_time_min',\n",
    "            'preferred_apptmnt_time_max', 'preferred_apptmnt_time_count', 'dignity_avg',\n",
    "            'dignity_min', 'dignity_max', 'dignity_count', 'involve_dec_avg', 'involve_dec_min',\n",
    "            'involve_dec_max', 'involve_dec_count', 'up_to_date_info_avg', 'up_to_date_info_min',\n",
    "            'up_to_date_info_max', 'up_to_date_info_count', \"index_letter\", \"ods\", \"lon\", 'lat',\n",
    "            'doctor_gender_ratio', 'town', 'county', 'postcode', 'last_verified_1', 'last_verified_2',\n",
    "            'doctor_names', 'ePrescription_available', \"reception_sun\", 'reception_mon',\n",
    "            'reception_tue', 'reception_wed', 'reception_thu', 'reception_fri', 'reception_sat',\n",
    "            'surgery_sun', 'surgery_mon', 'surgery_tue', 'surgery_wed', 'surgery_thu',\n",
    "            'surgery_fri', 'surgery_sat', 'parent_org', 'services', 'facilities',\n",
    "            'treatment_indicators'\n",
    "           ]\n",
    "    writer.writerow(row0)\n",
    "    for indeks, link in enumerate(links):\n",
    "        if indeks < 8 and indeks > 5:\n",
    "            print \"zakładka: \" + link[-19:-15]\n",
    "            gp_list_page = requests.get(link)\n",
    "            tree2 = html.fromstring(gp_list_page.content)\n",
    "            last_page = tree2.xpath(\"//a[@id='lastPage']/@href\")\n",
    "            if len(last_page) == 1:\n",
    "                last_page = str(last_page[0])\n",
    "                temp = last_page.find(\"page=\")\n",
    "                nr = int(last_page[temp+5:])\n",
    "                strony = range(1,nr+1)\n",
    "                for page_nr in strony:\n",
    "                    if page_nr > -1:\n",
    "                        print \"strona numer: \" + str(page_nr)\n",
    "                        page_link = link + '&page=' + str(page_nr)\n",
    "                        gp_list_page2 = requests.get(page_link)\n",
    "                        tree3 = html.fromstring(gp_list_page2.content)\n",
    "                        gp_links = tree3.xpath('//li/a/@href')\n",
    "                        for iii, gp_link in enumerate(gp_links):\n",
    "                            if iii > -1:\n",
    "                                wyniczek = kielbaska(gp_link)\n",
    "                                writer.writerow(wyniczek)\n",
    "                                if len(wyniczek) != 51:\n",
    "                                    print 'UWAGA - nieprawidłowa długość wpisu dla GP: ' + str(wyniczek[0])\n",
    "            elif len(last_page) == 0:\n",
    "                gp_links = tree2.xpath('//li/a/@href')\n",
    "                for iii, gp_link in enumerate(gp_links):\n",
    "                    if iii > -1:\n",
    "                        wyniczek = kielbaska(gp_link)\n",
    "                        writer.writerow(wyniczek)\n",
    "                        if len(wyniczek) != 51:\n",
    "                            print 'UWAGA - nieprawidłowa długość wpisu dla GP: ' + str(wyniczek[0])\n",
    "        else:\n",
    "            print \"UWAGA! Jedna ze stron NHS Choices jest inaczej sformatowana niż pozostałe\"\n",
    "            \n",
    "\n",
    "\n",
    "print\n",
    "print \"koniec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It may have been an ascii-encoded unicode string\n"
     ]
    }
   ],
   "source": [
    "# data=\"St Catherine’s Surgery\"\n",
    "# udata=data.decode(\"utf-8\")\n",
    "# print udata\n",
    "# asciidata=udata.encode(\"ascii\",\"xmlcharrefreplace\")\n",
    "# print asciidata\n",
    "\n",
    "mystring = \"St Catherine’s Surgery\"\n",
    "\n",
    "try:\n",
    "    mystring.decode('ascii')\n",
    "except UnicodeDecodeError:\n",
    "    print \"it was not a ascii-encoded unicode string\"\n",
    "else:\n",
    "    print \"It may have been an ascii-encoded unicode string\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
